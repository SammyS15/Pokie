{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.stats import gaussian_kde\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Set up path to import from src\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Import pokie from pokie.py\n",
    "from pokie import pokie, get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019fd141",
   "metadata": {},
   "source": [
    "# Section 3.2: Analyzing distribution shifts\n",
    "\n",
    "UPDATE This notebook defines the experiment for Section 3.2. Note that as noted in the paper, this the 100 Dimensional Case was run on a AMD CPU and not locally however we repeat the experiment but in lower dimensions such that it can be run on a Apple M2 with 8 GB Ram. To scale up to 100 Dimensions or beyond, update this variable to whatever dimensionality you want. ''n_dimensions''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_num_runs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2ddc3",
   "metadata": {},
   "source": [
    "# Sample from GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d670e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_from_gmm(means, covariances, n_samples):\n",
    "    \"\"\"\n",
    "    means:       shape (n_components, n_dimensions)\n",
    "    covariances: shape (n_components, n_dimensions),\n",
    "                 each row is the diagonal of the covariance matrix for that component\n",
    "    n_samples:   number of samples to generate in total\n",
    "    \"\"\"\n",
    "    n_components, n_dimensions = means.shape\n",
    "    samples = np.zeros((n_samples, n_dimensions))\n",
    "    # Assume uniform mixing weights for simplicity\n",
    "    component_choices = np.random.choice(\n",
    "        n_components, size=n_samples, p=np.ones(n_components)/n_components\n",
    "    )\n",
    "\n",
    "    for i, comp in enumerate(component_choices):\n",
    "        # Use np.diag(...) so each component's covariance is diagonal\n",
    "        cov = np.diag(covariances[comp])\n",
    "        samples[i, :] = np.random.multivariate_normal(means[comp], cov)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5715fa3",
   "metadata": {},
   "source": [
    "# 2 Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b5ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GMM parameters\n",
    "n_components = 20\n",
    "n_dimensions = 2\n",
    "n_truth_samples = 5000  # Number of ground truth samples\n",
    "num_posterior_samples = 5000  # Number of posterior samples per truth\n",
    "epsilon = 1e-3  # Small value to avoid division by zero\n",
    "\n",
    "# Initialize random means and covariances for the GMM components\n",
    "means = np.random.rand(n_components, n_dimensions) * 10  # Random means between 0 and 10\n",
    "covariances = np.random.rand(n_components, n_dimensions) + epsilon  # Ensure strictly positive variance\n",
    "\n",
    "# Generate the truth data (no shift)\n",
    "truth_data = generate_samples_from_gmm(means, covariances, n_truth_samples)\n",
    "\n",
    "# Generate the models with different shift magnitudes from -10 to 10 along the diagonal\n",
    "shift_magnitudes = np.arange(-10, 11, 1)  # Shift values from -10 to 10  # Shift values from -10 to 10\n",
    "# shift_magnitudes = np.arange(-3, 4, 1)  # Shift values from -3 to 3  # Shift values from -10 to 10\n",
    "shift_magnitudes = [-6, -3, 0, 3, 6]  # Example shift magnitudes for testing\n",
    "num_models = len(shift_magnitudes)\n",
    "\n",
    "print(f'Shift magnitudes: {shift_magnitudes}')\n",
    "\n",
    "# Generate shifted GMMs once per model\n",
    "models_base = np.zeros((num_models, num_posterior_samples, n_dimensions))  # Shape: (21, 500, 2)\n",
    "\n",
    "for i, shift in enumerate(tqdm(shift_magnitudes, desc=f'Generating Shifted GMMs')):\n",
    "    models_base[i] = generate_samples_from_gmm(means, covariances, num_posterior_samples) + np.ones(n_dimensions) * shift\n",
    "\n",
    "# Now, expand to (num_models, num_truth_samples, num_posterior_samples, dimensions)\n",
    "models = np.repeat(models_base[:, np.newaxis, :, :], n_truth_samples, axis=1)\n",
    "\n",
    "# Validate shape\n",
    "assert models.shape == (num_models, n_truth_samples, num_posterior_samples, n_dimensions), \"Shape mismatch!\"\n",
    "\n",
    "epsilon = 1e-10  # Small value to avoid division by zero\n",
    "\n",
    "# Get min and max from truth_data (per dimension)\n",
    "low = np.min(truth_data, axis=0, keepdims=True)  # Shape: (1, n_dimensions)\n",
    "high = np.max(truth_data, axis=0, keepdims=True)  # Shape: (1, n_dimensions)\n",
    "\n",
    "# Normalize truth_data\n",
    "truth_data_normalized = (truth_data - low) / (high - low + epsilon)\n",
    "\n",
    "# Normalize models (loop over each shift magnitude)\n",
    "models_normalized = np.zeros_like(models)\n",
    "for m_idx in range(num_models):\n",
    "    models_normalized[m_idx] = (models[m_idx] - low) / (high - low + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617604df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_full_overlay(models_normalized, truth_data_normalized, shift_magnitudes):\n",
    "    \"\"\"\n",
    "    Overlay all ground truth samples and all posterior samples (for all shifts).\n",
    "    Color encodes shift level. Ground truth shown in red.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    num_models = len(shift_magnitudes)\n",
    "    n_truth_samples = truth_data_normalized.shape[0]\n",
    "    colors = [\n",
    "        '#440154',  # deep purple\n",
    "        '#3b528b',  # indigo blue\n",
    "        '#21918c',  # teal\n",
    "        '#5ec962',  # green\n",
    "        \"#b9663c\",  # burnt orange\n",
    "    ]\n",
    "\n",
    "    # Plot posterior samples for each shift\n",
    "    for i, shift in enumerate(shift_magnitudes):\n",
    "        samples = models_normalized[i, 0, :, :]\n",
    "        ax.scatter(samples[:, 0], samples[:, 1],\n",
    "                   alpha=1.0, s=5, color=colors[i], label=f'Shift: {shift:+d}')\n",
    "    \n",
    "    # Plot all ground truth samples\n",
    "    ax.scatter(truth_data_normalized[:, 0], truth_data_normalized[:, 1],\n",
    "               color='red', s=30, marker='*', edgecolor='black', linewidth=0.5,\n",
    "               label='Ground Truth', zorder=10)\n",
    "\n",
    "    # Aesthetic settings\n",
    "    # ax.set_xlim(0, 1)\n",
    "    # ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    # ax.set_xlabel('Dimension 1 (normalized)', fontsize=20)\n",
    "    # ax.set_ylabel('Dimension 2 (normalized)', fontsize=20)\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "    # ax.set_title('Posterior Samples Across Shifts (All GT)', fontsize=15, fontweight='bold')\n",
    "    # Shared legend\n",
    "    ax.legend(loc='lower right', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig2D = gmm_full_overlay(models_normalized, truth_data_normalized, shift_magnitudes)\n",
    "plt.show()\n",
    "fig2D.savefig('/Users/sammysharief/Downloads/Pokie/Plots/GMM/Paper_Plot/GMM_Distribution_Plot.png', dpi = 300, bbox_inches='tight')\n",
    "fig2D.savefig('/Users/sammysharief/Downloads/Pokie/Plots/GMM/Paper_Plot/GMM_Distribution_Plot.pdf', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeab660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify your device (CUDA > MPS > CPU)\n",
    "device = get_device()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Convert to torch Tensors on the chosen device\n",
    "truth_data_normalized   = torch.tensor(truth_data_normalized, dtype=torch.float32, device=device)\n",
    "models_normalized = torch.tensor(models_normalized,   dtype=torch.float32, device=device)\n",
    "\n",
    "probability, quality, n_over_N_vals = pokie(\n",
    "    truth_data_normalized, models_normalized, num_runs=curr_num_runs\n",
    ")\n",
    "\n",
    "# Convert results, calibrated, n_over_N_vals back to numpy arrays\n",
    "probability = probability.cpu().numpy()\n",
    "quality = quality.cpu().numpy()\n",
    "n_over_N_vals = n_over_N_vals.cpu().numpy()\n",
    "\n",
    "print('\\nShift Magnitudes:', shift_magnitudes)\n",
    "print(\"Pokie Probability:\", probability)\n",
    "print(\"Pokie Quality:\", quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab1bb2",
   "metadata": {},
   "source": [
    "# 20 Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28235817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GMM parameters\n",
    "n_components = 20\n",
    "n_dimensions = 20\n",
    "n_truth_samples = 500  # Number of ground truth samples\n",
    "num_posterior_samples = 500  # Number of posterior samples per truth\n",
    "epsilon = 1e-3  # Small value to avoid division by zero\n",
    "\n",
    "# Initialize random means and covariances for the GMM components\n",
    "means = np.random.rand(n_components, n_dimensions) * 10  # Random means between 0 and 10\n",
    "covariances = np.random.rand(n_components, n_dimensions) + epsilon  # Ensure strictly positive variance\n",
    "\n",
    "# Generate the truth data (no shift)\n",
    "truth_data = generate_samples_from_gmm(means, covariances, n_truth_samples)\n",
    "\n",
    "# Generate the models with different shift magnitudes from -10 to 10 along the diagonal\n",
    "# shift_magnitudes = np.arange(-10, 11, 1)  # Shift values from -10 to 10  # Shift values from -10 to 10\n",
    "shift_magnitudes = [-6, -3, 0, 3, 6]  # Example shift magnitudes for testing\n",
    "num_models = len(shift_magnitudes)\n",
    "\n",
    "print(f'Shift magnitudes: {shift_magnitudes}')\n",
    "\n",
    "# Generate shifted GMMs once per model\n",
    "models_base = np.zeros((num_models, num_posterior_samples, n_dimensions))  # Shape: (21, 500, 2)\n",
    "\n",
    "for i, shift in enumerate(tqdm(shift_magnitudes, desc=f'Generating Shifted GMMs')):\n",
    "    models_base[i] = generate_samples_from_gmm(means, covariances, num_posterior_samples) + np.ones(n_dimensions) * shift\n",
    "\n",
    "# Now, expand to (num_models, num_truth_samples, num_posterior_samples, dimensions)\n",
    "models = np.repeat(models_base[:, np.newaxis, :, :], n_truth_samples, axis=1)\n",
    "\n",
    "# Validate shape\n",
    "assert models.shape == (num_models, n_truth_samples, num_posterior_samples, n_dimensions), \"Shape mismatch!\"\n",
    "\n",
    "epsilon = 1e-10  # Small value to avoid division by zero\n",
    "\n",
    "# Get min and max from truth_data (per dimension)\n",
    "low = np.min(truth_data, axis=0, keepdims=True)  # Shape: (1, n_dimensions)\n",
    "high = np.max(truth_data, axis=0, keepdims=True)  # Shape: (1, n_dimensions)\n",
    "\n",
    "# Normalize truth_data\n",
    "truth_data_normalized = (truth_data - low) / (high - low + epsilon)\n",
    "\n",
    "# Normalize models (loop over each shift magnitude)\n",
    "models_normalized = np.zeros_like(models)\n",
    "for m_idx in range(num_models):\n",
    "    models_normalized[m_idx] = (models[m_idx] - low) / (high - low + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify your device (CUDA > MPS > CPU)\n",
    "device = get_device()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Convert to torch Tensors on the chosen device\n",
    "truth_data_normalized   = torch.tensor(truth_data_normalized, dtype=torch.float32, device=device)\n",
    "models_normalized = torch.tensor(models_normalized,   dtype=torch.float32, device=device)\n",
    "\n",
    "probability, quality, n_over_N_vals = pokie(\n",
    "    truth_data_normalized, models_normalized, num_runs=curr_num_runs\n",
    ")\n",
    "\n",
    "# Convert results, calibrated, n_over_N_vals back to numpy arrays\n",
    "probability = probability.cpu().numpy()\n",
    "quality = quality.cpu().numpy()\n",
    "n_over_N_vals = n_over_N_vals.cpu().numpy()\n",
    "\n",
    "print('\\nShift Magnitudes:', shift_magnitudes)\n",
    "print(\"Pokie Probability:\", probability)\n",
    "print(\"Pokie Quality:\", quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed34d5",
   "metadata": {},
   "source": [
    "# 100 Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GMM parameters\n",
    "n_components = 20\n",
    "n_dimensions = 100\n",
    "n_truth_samples = 500  # Number of ground truth samples\n",
    "num_posterior_samples = 500  # Number of posterior samples per truth\n",
    "epsilon = 1e-3  # Small value to avoid division by zero\n",
    "\n",
    "# Initialize random means and covariances for the GMM components\n",
    "means = np.random.rand(n_components, n_dimensions) * 10  # Random means between 0 and 10\n",
    "covariances = np.random.rand(n_components, n_dimensions) + epsilon  # Ensure strictly positive variance\n",
    "\n",
    "# Generate the truth data (no shift)\n",
    "truth_data = generate_samples_from_gmm(means, covariances, n_truth_samples)\n",
    "\n",
    "# Generate the models with different shift magnitudes from -10 to 10 along the diagonal\n",
    "# shift_magnitudes = np.arange(-10, 11, 1)  # Shift values from -10 to 10  # Shift values from -10 to 10\n",
    "shift_magnitudes = [-6, -3, 0, 3, 6]  # Example shift magnitudes for testing\n",
    "num_models = len(shift_magnitudes)\n",
    "\n",
    "print(f'Shift magnitudes: {shift_magnitudes}')\n",
    "\n",
    "# Generate shifted GMMs once per model\n",
    "models_base = np.zeros((num_models, num_posterior_samples, n_dimensions))  # Shape: (21, 500, 2)\n",
    "\n",
    "for i, shift in enumerate(tqdm(shift_magnitudes, desc=f'Generating Shifted GMMs')):\n",
    "    models_base[i] = generate_samples_from_gmm(means, covariances, num_posterior_samples) + np.ones(n_dimensions) * shift\n",
    "\n",
    "# Now, expand to (num_models, num_truth_samples, num_posterior_samples, dimensions)\n",
    "models = np.repeat(models_base[:, np.newaxis, :, :], n_truth_samples, axis=1)\n",
    "\n",
    "# Validate shape\n",
    "assert models.shape == (num_models, n_truth_samples, num_posterior_samples, n_dimensions), \"Shape mismatch!\"\n",
    "\n",
    "epsilon = 1e-10  # Small value to avoid division by zero\n",
    "\n",
    "# Get min and max from truth_data (per dimension)\n",
    "low = np.min(truth_data, axis=0, keepdims=True)  # Shape: (1, n_dimensions)\n",
    "high = np.max(truth_data, axis=0, keepdims=True)  # Shape: (1, n_dimensions)\n",
    "\n",
    "# Normalize truth_data\n",
    "truth_data_normalized = (truth_data - low) / (high - low + epsilon)\n",
    "\n",
    "# Normalize models (loop over each shift magnitude)\n",
    "models_normalized = np.zeros_like(models)\n",
    "for m_idx in range(num_models):\n",
    "    models_normalized[m_idx] = (models[m_idx] - low) / (high - low + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c728bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify your device (CUDA > MPS > CPU)\n",
    "device = get_device()\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Convert to torch Tensors on the chosen device\n",
    "truth_data_normalized   = torch.tensor(truth_data_normalized, dtype=torch.float32, device=device)\n",
    "models_normalized = torch.tensor(models_normalized,   dtype=torch.float32, device=device)\n",
    "\n",
    "probability, quality, n_over_N_vals = pokie(\n",
    "    truth_data_normalized, models_normalized, num_runs=curr_num_runs\n",
    ")\n",
    "\n",
    "# Convert results, calibrated, n_over_N_vals back to numpy arrays\n",
    "probability = probability.cpu().numpy()\n",
    "quality = quality.cpu().numpy()\n",
    "n_over_N_vals = n_over_N_vals.cpu().numpy()\n",
    "\n",
    "print('\\nShift Magnitudes:', shift_magnitudes)\n",
    "print(\"Pokie Probability:\", probability)\n",
    "print(\"Pokie Quality:\", quality)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tarp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
